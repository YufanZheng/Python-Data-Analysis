{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WSDM - KKBox's Music Recommendation Challenge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+-----------------+-------------------+---------------+------+\n",
      "| msno|song_id|source_system_tab| source_screen_name|    source_type|target|\n",
      "+-----+-------+-----------------+-------------------+---------------+------+\n",
      "| 9176| 474849|          explore|            Explore|online-playlist|     1|\n",
      "|19273|1425656|       my library|Local playlist more| local-playlist|     1|\n",
      "|19273| 768950|       my library|Local playlist more| local-playlist|     1|\n",
      "|19273| 150624|       my library|Local playlist more| local-playlist|     1|\n",
      "| 9176| 210388|          explore|            Explore|online-playlist|     1|\n",
      "+-----+-------+-----------------+-------------------+---------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the zip csv file\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "# Load data and have a look\n",
    "df = sqlContext.read \\\n",
    "        .format('com.databricks.spark.csv') \\\n",
    "        .options(header='true', inferschema='true') \\\n",
    "        .load('./process/train.csv.gz')\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's firstly check if the dataset if balanced\n",
    "- From the result, we find that the positive and negative occupy almost the same\n",
    "- So we don't need to rebalance the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive: 0.5035170841614234 \n",
      "Negative: 0.49648291583857657\n"
     ]
    }
   ],
   "source": [
    "total    = df.count()\n",
    "positive = df.filter(df['target']==1).count()\n",
    "negative = df.filter(df['target']==0).count()\n",
    "\n",
    "print(\"Positive: {} \\nNegative: {}\".format(float(positive)/total, float(negative)/total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random split the data into train and eval\n",
    "- Train: 0.8\n",
    "- Eval : 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainDF, evalDF = df.randomSplit([0.8, 0.2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load data and have a look\n",
    "testDF = sqlContext.read \\\n",
    "        .format('com.databricks.spark.csv') \\\n",
    "        .options(header='true', inferschema='true') \\\n",
    "        .load('./process/test.csv.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Method 1: Collaborative Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We will only choose three columns to build the model\n",
    "- msno: user_id\n",
    "- song_id\n",
    "- target: score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root-mean-square error = 0.47410587523568726\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Build and train the model\n",
    "als = ALS(maxIter=15, regParam=0.01, userCol=\"msno\", itemCol=\"song_id\", ratingCol=\"target\")\n",
    "model = als.fit(trainDF)\n",
    "\n",
    "# Predict and evaluate on test dataset\n",
    "predictions = model.transform(evalDF)\n",
    "\n",
    "# Fill the NaN prediction with 0.5\n",
    "predictions = predictions.fillna(0.5)\n",
    "\n",
    "# Evaluation\n",
    "evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"target\",\n",
    "                                predictionCol=\"prediction\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root-mean-square error = \" + str(rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+\n",
      "| id|  prediction|\n",
      "+---+------------+\n",
      "|  0|  0.38287437|\n",
      "|  1|  0.26514986|\n",
      "|  2| -0.21284494|\n",
      "|  3|4.9685687E-4|\n",
      "|  4|  0.16973872|\n",
      "|  5|  0.45950592|\n",
      "|  6| 0.120891124|\n",
      "|  7|  0.60329115|\n",
      "|  8|   0.4835725|\n",
      "|  9|  0.79836446|\n",
      "| 10|  0.83793026|\n",
      "| 11|  0.25893503|\n",
      "| 12|  0.24025439|\n",
      "| 13|  0.42867026|\n",
      "| 14|   0.2588826|\n",
      "| 15|   0.2154228|\n",
      "| 16|  0.26846784|\n",
      "| 17|   0.4095208|\n",
      "| 18|   0.7259852|\n",
      "| 19|   0.8428296|\n",
      "+---+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Predict and evaluate on test dataset\n",
    "resultDF_1 = model.transform(testDF)\n",
    "resultDF_1 = resultDF_1.select([\"id\", \"prediction\"])\n",
    "resultDF_1 = resultDF_1.sort('id')\n",
    "resultDF_1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Method 2: Classification and Regression "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select and merge all user information from all data frame\n",
    "- train, eval and test\n",
    "    - msno\n",
    "    - song_id\n",
    "    - source_system_tab\n",
    "    - source_screen_name\n",
    "    - source_type\n",
    "    - target\n",
    "- user: \n",
    "    - msno\n",
    "    - city\n",
    "    - bd\n",
    "    - gender\n",
    "    - registered_via\n",
    "- song: \n",
    "    - song_id\n",
    "    - song_length : Process by diving to minutes\n",
    "    - genre_ids\n",
    "    - artist_name\n",
    "    - composer\n",
    "    - lyricist\n",
    "    - language\n",
    "- song_extra:\n",
    "    - song_id          \n",
    "    - name      \n",
    "    - isrc : Process to get the country code"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
